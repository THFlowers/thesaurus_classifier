#!/usr/bin/env python3

import json
import sys

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
# from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import PorterStemmer

import wordsegment

# Global data to eliminate construction and destruction again and again
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))


# Refactored out of original code, now applying strategy pattern
# Functor that queries a similarity database with format (word1 : str, word2 : str, similarity_score : float)
# Supply Database connection, originally used with an SQLite3 database called 'SEWordSim-r1.db'
# similarity_score between 0 and 1
class SimDB(object):
    def __init__(self, sim_db_conn):
        self.db_conn = sim_db_conn

    def __call__(self, wd, num_similar, min_similarity, max_similarity=1.0):
        self.db_conn.execute("select term_2 from Word_Similarity where term_1=\'" + wd + "\'"
                             + " and " + str(min_similarity) + " <= similarity <= " + str(max_similarity)
                             + " order by similarity "
                             + " limit " + str(num_similar) + ";")
        res = self.db_conn.fetchall()
        return [x[0] if isinstance(x, tuple) else x for x in res]


class Word2Vec(object):
    def __init__(self, model, index=None):
        self.model = model
        self.index = index

    def __call__(self, wd, num_similar, min_similarity, max_similarity=1.0):
        assert(0 <= num_similar <= 10)
        if not wd.isalpha():
            return []

        try:
            ret = self.model.most_similar(positive=wd, topn=num_similar, indexer=self.index)
        except KeyError:
            return []

        return [word for (word, score) in ret if min_similarity <= score <= max_similarity]


# term/doc_frequencies are dictionaries as generated by make_model
# this program will load these in main from provided files,
# other programs importing this code must load/supply these manually

# Calculate the TFidF for a given word and category (viewed as a document)
# Functor such that term/doc frequencies are:
# 1) bundled with function
# 2) can be retrieved (used to be a closure)
class TFidF(object):
    def __init__(self, term_freqs, doc_freqs):
        self.term_frequencies = term_freqs
        self.doc_frequencies = doc_freqs

    # word is a string, category is a string and must be a valid category name
    def __call__(self, word, category):
        try:
            term_freq = self.term_frequencies[category]['counts'][word]
            # num_docs = self.term_frequencies[category]['num_docs']
            doc_freq = self.doc_frequencies[word]
        except KeyError:
            return 0

        if doc_freq == 0:
            return 0
        else:
            return term_freq * (1 / doc_freq)

    def get_categories(self):
        return self.term_frequencies.keys()

    def get_term_frequencies(self):
        return self.term_frequencies

    def get_doc_frequencies(self):
        return self.doc_frequencies


# TFidF must be a functor that takes two strings, word and category
# Also must have a get_categories() method
# returns pair (category name : string, score : float)
#
# Possible concern, examples are stemmed so should I stem the input here?
# Answer: Data in SEWordSim DB is stemmed, so yes
# Now what about word2vec?
def classify(tf_idf, message, sim_func=None, num_similar=1, min_similarity=0.3, stemmed_database=True):
    assert(num_similar >= 0)
    assert(0.0 <= min_similarity <= 100.0)

    # NOTE: 2/27/20, added fixes like in make_model
    words = word_tokenize(message.lower().strip())

    segmented_words = []
    for wd in words:  # example 'artstation' --> ['art', 'station']
        segmented_words.append(wd)
        segments = wordsegment.segment(wd)
        if len(segments) > 1:
           segmented_words.extend(segments)

    del words

    # NOTE: 2/19/20, discovered typo, was message, not words (now segmented_words), 1% filter hit 46% match rate
    filtered = [wd for wd in segmented_words
                if wd not in stop_words and 0 < sum(map((lambda x: 1 if x[1].isalnum() else 0), enumerate(wd)))]

    del segmented_words

    # For each term query once for num_similar and min_similarity
    # If term not found in model data, and not found in subs, then ignore it (score of 0)
    subs = {}
    for wd in filtered:
        sim_words = [stemmer.stem(wd)]
        if sim_func is not None and num_similar > 0 and wd.isalnum():
            if stemmed_database:
                lkup = sim_words[0]
            else:
                lkup = wd

            res = sim_func(lkup, num_similar, min_similarity)
            if res:
                # convoluted code to avoid duplicates
                sim_words.extend(w for w in map(stemmer.stem, res) if w not in sim_words)

        if wd in subs:
            subs[wd]["count"] += 1
        else:
            subs[wd] = {"words": sim_words, "count": 1}

    # TODO: maybe put a "merge" phase here, if one of the subs for a term is also a term, then combine their counts
    # NOTE: "merge" code was never hit in DB implementation, so either loops+test are wrong, or just very unlucky
    # merged = deepcopy(subs)
    # for sub in subs:
    #    for wd in subs[sub]["words"]:
    #        if wd in subs:
    #            merged[sub] = {"words": set(subs[sub]["words"] + subs[wd]["words"]).remove(wd),
    #                          "count": subs[sub]["count"] + subs[wd]["words"]}
    #            del merged[wd]

    # dictionary of words and number of times they occur in the message
    similar_scores = {}

    for cur_cat in tf_idf.get_categories():
        score = 0
        for wd in filtered:
            # term_freq = tf_idf.get_term_frequencies()[cur_cat]['counts'][wd]
            # if term_freq != 0:
            #    score += subs[wd]["count"] * tf_idf(wd, cur_cat)
            # if sim_func is not None and num_similar > 0:
            score += sum(map((lambda sub: subs[wd]["count"] * tf_idf(sub, cur_cat)),
                             subs[wd]["words"]))

        similar_scores[cur_cat] = score

    del filtered
    best_cat = max(similar_scores, key=lambda k: similar_scores[k])

    if similar_scores[best_cat] == 0:
        return None, 0
    else:
        return best_cat, similar_scores[best_cat]


if __name__ == "__main__":
    if len(sys.argv) != 4:
        print("Usage: ./categorize term_frequencies.json doc_frequencies.json message.txt")
        exit(1)

    wordsegment.load()

    with open(sys.argv[1], 'r') as fi:
        term_frequencies = json.load(fi)

    with open(sys.argv[2], 'r') as fi:
        doc_frequencies = json.load(fi)

    with open(sys.argv[3], 'r') as fi:
        new_message = fi.read()

    my_idF = TFidF(term_frequencies, doc_frequencies)
    cat = classify(my_idF, new_message.lower())
    print('Category: "', cat[0], '" Score: ', cat[1], sep='')
